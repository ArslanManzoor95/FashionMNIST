{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: In PyTorch, a Dataset is an object that holds your data and is responsible for loading it from disk or another source. It's a collection of data samples, and each sample is usually a tuple (data, label).\n",
    "\n",
    "DataLoader: The DataLoader takes a Dataset as input and makes it iterable, meaning that you can loop over it. It handles loading the data in batches, which is important because it's more efficient to process data in small groups rather than individually or all at once. This is especially true when training neural networks.\n",
    "\n",
    "Batching: This refers to taking a subset of your dataset and processing it at one time. Here, batch_size is set to 64, which means the DataLoader will give you 64 samples each time you iterate over it. Each sample consists of features and a label.\n",
    "\n",
    "Multiprocess Data Loading: This is an efficiency feature of DataLoader. It can use multiple processes to load data in parallel, which speeds up the process significantly, especially when dealing with large datasets.\n",
    "\n",
    "The Loop: The for loop in your code is where you would typically put your model training code. Each iteration of the loop gives you a batch of data (X) and labels (Y). The X.shape and Y.shape are the dimensions of the data and labels tensor respectively.\n",
    "\n",
    "X [N, C, H, W]: This is the shape of the data tensor. N is the batch size (64 in your case), C is the number of channels (1 for grayscale images, 3 for RGB color images), H is the height of the image, and W is the width of the image.\n",
    "Y: This is the shape of the labels tensor. It's just [N] because there's one label per image.\n",
    "\n",
    "The break statement stops the loop after the first batch, which is often used for testing to make sure that the data loading is working as expected without having to iterate over the entire dataset.\n",
    "\n",
    "Each time you run this loop, the DataLoader will automatically handle the fetching and transforming of the data, allowing you to focus on implementing and training your model. This setup is fundamental when working with neural networks as it enables efficient and manageable data handling.\n",
    "\n",
    "\n",
    "In the for loop you're referring to, X and y represent two different, but related, components of your dataset:\n",
    "\n",
    "X is commonly used to denote the input features of your data. In the context of image processing, X would be the actual image data that your model will learn from. If you're dealing with a batch of images, X would be a tensor containing several images, each represented as a grid of pixel values.\n",
    "\n",
    "y is commonly used to denote the labels or targets associated with your input data. For supervised learning, where the goal is to predict a label given some input, y would contain the correct answers or the ground truth for each input sample in X. For instance, if you're working on a digit classification task, y would contain the actual digit (0 through 9) that each image in X represents.\n",
    "\n",
    "The DataLoader combines your dataset's input features and labels into batches. When you iterate over the DataLoader, it yields pairs of (X, y) for each batch. This is a tuple where the first element is the batch of input features and the second element is the corresponding batch of labels.\n",
    "\n",
    "Summary:\n",
    "\n",
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device Setup: Before defining a model, the code snippet is setting up the device on which the computations will run. PyTorch allows you to run your computations on a Graphics Processing Unit (GPU), which can greatly accelerate the training of deep learning models. The code checks if CUDA (NVIDIA's GPU computing API) is available. If it is, it will use the GPU; otherwise, it falls back to the CPU. More recently, PyTorch has added support for Apple's Metal Performance Shaders (MPS) to run on Apple Silicon (M1/M2 chips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
