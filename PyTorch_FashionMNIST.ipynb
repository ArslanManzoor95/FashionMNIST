{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: In PyTorch, a Dataset is an object that holds your data and is responsible for loading it from disk or another source. It's a collection of data samples, and each sample is usually a tuple (data, label).\n",
    "\n",
    "DataLoader: The DataLoader takes a Dataset as input and makes it iterable, meaning that you can loop over it. It handles loading the data in batches, which is important because it's more efficient to process data in small groups rather than individually or all at once. This is especially true when training neural networks.\n",
    "\n",
    "Batching: This refers to taking a subset of your dataset and processing it at one time. Here, batch_size is set to 64, which means the DataLoader will give you 64 samples each time you iterate over it. Each sample consists of features and a label.\n",
    "\n",
    "Multiprocess Data Loading: This is an efficiency feature of DataLoader. It can use multiple processes to load data in parallel, which speeds up the process significantly, especially when dealing with large datasets.\n",
    "\n",
    "The Loop: The for loop in your code is where you would typically put your model training code. Each iteration of the loop gives you a batch of data (X) and labels (Y). The X.shape and Y.shape are the dimensions of the data and labels tensor respectively.\n",
    "\n",
    "X [N, C, H, W]: This is the shape of the data tensor. N is the batch size (64 in your case), C is the number of channels (1 for grayscale images, 3 for RGB color images), H is the height of the image, and W is the width of the image.\n",
    "Y: This is the shape of the labels tensor. It's just [N] because there's one label per image.\n",
    "\n",
    "The break statement stops the loop after the first batch, which is often used for testing to make sure that the data loading is working as expected without having to iterate over the entire dataset.\n",
    "\n",
    "Each time you run this loop, the DataLoader will automatically handle the fetching and transforming of the data, allowing you to focus on implementing and training your model. This setup is fundamental when working with neural networks as it enables efficient and manageable data handling.\n",
    "\n",
    "\n",
    "In the for loop you're referring to, X and y represent two different, but related, components of your dataset:\n",
    "\n",
    "X is commonly used to denote the input features of your data. In the context of image processing, X would be the actual image data that your model will learn from. If you're dealing with a batch of images, X would be a tensor containing several images, each represented as a grid of pixel values.\n",
    "\n",
    "y is commonly used to denote the labels or targets associated with your input data. For supervised learning, where the goal is to predict a label given some input, y would contain the correct answers or the ground truth for each input sample in X. For instance, if you're working on a digit classification task, y would contain the actual digit (0 through 9) that each image in X represents.\n",
    "\n",
    "The DataLoader combines your dataset's input features and labels into batches. When you iterate over the DataLoader, it yields pairs of (X, y) for each batch. This is a tuple where the first element is the batch of input features and the second element is the corresponding batch of labels.\n",
    "\n",
    "Summary:\n",
    "\n",
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device Setup: Before defining a model, the code snippet is setting up the device on which the computations will run. PyTorch allows you to run your computations on a Graphics Processing Unit (GPU), which can greatly accelerate the training of deep learning models. The code checks if CUDA (NVIDIA's GPU computing API) is available. If it is, it will use the GPU; otherwise, it falls back to the CPU. More recently, PyTorch has added support for Apple's Metal Performance Shaders (MPS) to run on Apple Silicon (M1/M2 chips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"using device {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, defining a model involves creating a class that inherits from nn.Module. This class framework provides a lot of built-in functionality that is necessary for neural networks, such as methods for moving the model to different devices (CPU or GPU), methods to set up the weights, and more.\n",
    "\n",
    "Here's how the class concepts apply to your PyTorch model:\n",
    "\n",
    "The NeuralNetwork class inherits from nn.Module, which means it gets all the functionality of nn.Module on top of what you define.\n",
    "__init__ is the constructor where you define the layers of your neural network.\n",
    "forward is a special method that defines the forward pass of your model. When you pass an input through your model, PyTorch will automatically call this method.\n",
    "By defining your neural network as a class, you're able to create instances of this network that have their own weights and biases, which can be trained independently of other instances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU or MPS if available.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below Code explained step-by-step\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "This line defines a new class called NeuralNetwork, which inherits from nn.Module. In PyTorch, nn.Module is a base class for all neural network modules, and your custom models should also subclass this. This inheritance gives your model access to a lot of functionality and utilities provided by PyTorch.\n",
    "\n",
    "def __init__(self):\n",
    "This is the initializer for your NeuralNetwork class. It is called when you create an instance of the NeuralNetwork.\n",
    "\n",
    "super().__init__()\n",
    "This line calls the initializer of the base class (nn.Module). This is necessary to properly set up the internals of the module.\n",
    "\n",
    "self.flatten = nn.Flatten()\n",
    "Here, you're defining a layer that will flatten the input. nn.Flatten() is a PyTorch layer that collapses all dimensions of the input except the batch dimension. It's commonly used to convert 2D images (like in the MNIST dataset) into a 1D tensor before feeding it into a fully connected layer.\n",
    "\n",
    "self.linear_relu_stack = nn.Sequential(\n",
    "This line starts the definition of a sequence of layers. nn.Sequential is a container that will process the input sequentially, passing it through each layer in the order they are added.\n",
    "\n",
    "nn.Linear(28*28, 512),\n",
    "This is the first layer in your sequence. nn.Linear is a fully connected linear layer. Here, it takes an input size of 28*28 (which is the size of a flattened MNIST image) and outputs a tensor of size 512.\n",
    "\n",
    "nn.ReLU(),\n",
    "This adds a Rectified Linear Unit (ReLU) activation function. Activation functions like ReLU are used to introduce non-linearities into the network, which are crucial for learning complex patterns.\n",
    "\n",
    "nn.Linear(512, 512),\n",
    "Another linear layer that takes the 512 inputs from the previous layer and outputs another 512.\n",
    "\n",
    "nn.ReLU(),\n",
    "Another ReLU activation function.\n",
    "\n",
    "nn.Linear(512, 10)\n",
    "The final linear layer that reduces the tensor from 512 to 10, which is the number of classes in a typical classification task like MNIST (digits 0-9).\n",
    "\n",
    "def forward(self, x):\n",
    "This method defines the forward pass of your network. It takes an input tensor x.\n",
    "\n",
    "x = self.flatten(x)\n",
    "The input x is passed through the flatten layer defined earlier.\n",
    "\n",
    "logits = self.linear_relu_stack(x)\n",
    "Then, the flattened x is passed through the sequence of linear and ReLU layers.\n",
    "\n",
    "return logits\n",
    "The output logits (unnormalized probability distributions) is returned.\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "Here, an instance of your NeuralNetwork is created and moved to the specified device (like a CPU or GPU).\n",
    "\n",
    "print(model)\n",
    "Finally, the model's structure is printed. This is useful for debugging and understanding the architecture of your model.\n",
    "\n",
    "This is a typical structure for a simple feedforward neural network in PyTorch. It's designed to process input data, apply a series of transformations and activations, and produce output that can be used for tasks like classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise Model Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, we need a loss function and an optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the PyTorch tutorial, you're setting up the components needed for training the neural network: the loss function and the optimizer. Let's break down these two lines:\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "This line initializes the loss function that you will use to evaluate how well your model is performing during training. In PyTorch, nn.CrossEntropyLoss is a commonly used loss function for classification tasks.\n",
    "\n",
    "Cross-Entropy Loss: This loss function measures the difference between two probability distributions - the actual labels and the predictions from the model. It's particularly suitable for problems like multi-class classification (like digit classification in MNIST).\n",
    "\n",
    "Working of Cross-Entropy Loss: It calculates the loss by considering the model's output as probabilities (using softmax) and comparing them with the true distribution (the actual labels). The goal of training is to minimize this loss, bringing the model's predictions closer to the actual labels.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "Here, you're defining the optimizer, which is the algorithm used to update the model's parameters (weights and biases) during training based on the gradients computed during backpropagation.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): torch.optim.SGD stands for Stochastic Gradient Descent. It's a very basic yet powerful optimization algorithm used in training neural networks.\n",
    "\n",
    "Parameters: model.parameters() passes all the trainable parameters of your model to the optimizer. These are the variables that will be adjusted to minimize the loss function.\n",
    "\n",
    "Learning Rate (lr): 1e-3 or 0.001 is the learning rate. It determines the step size at each iteration while moving towards a minimum of the loss function. A smaller learning rate requires more training epochs through the training dataset, whereas a larger learning rate results in rapid changes and requires fewer training epochs.\n",
    "\n",
    "The choice of loss function and optimizer, and their parameters like learning rate, are crucial in determining how effectively your model learns from the training data. Fine-tuning these can significantly impact the performance of your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters.\n",
    "\n",
    "This code snippet defines a function for training the neural network. It's a crucial part of the learning process where the model's parameters are updated based on the data. \n",
    "\n",
    "step by step:\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "This line defines the train function with four parameters:\n",
    "\n",
    "dataloader: This provides batches of training data.\n",
    "model: The neural network model that you're training.\n",
    "loss_fn: The loss function used to evaluate the model's performance.\n",
    "optimizer: The optimization algorithm used to update model parameters.\n",
    "size = len(dataloader.dataset)\n",
    "\n",
    "This calculates the total number of samples in the dataset.\n",
    "\n",
    "model.train()\n",
    "\n",
    "Puts the model in training mode. This is important because some models may behave differently during training than during testing (e.g., dropout is used during training but not during testing).\n",
    "\n",
    "for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "This loop iterates over the dataloader. In each iteration, it provides a batch of data (X) and the corresponding labels (y).\n",
    "\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "This line moves the data and labels to the device (like a CPU or GPU), which you specified earlier.\n",
    "\n",
    "pred = model(X)\n",
    "\n",
    "Here, the model makes predictions based on the batch of data X.\n",
    "\n",
    "loss = loss_fn(pred, y)\n",
    "\n",
    "The loss function calculates the loss, comparing the predictions pred to the actual labels y.\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "This line performs backpropagation. It computes the gradient of the loss with respect to all the weights in the model.\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "This line updates the weights of the model using the gradients computed by loss.backward().\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "Resets the gradients of the model parameters. It's important to clear them before the next batch otherwise the gradients will accumulate.\n",
    "\n",
    "if batch % 100 == 0:\n",
    "This conditional statement is for logging. It prints the current loss and the number of samples processed so far after every 100 batches.\n",
    "\n",
    "loss, current = loss.item(), (batch + 1) * len(X)\n",
    "This line extracts the loss value as a Python float and calculates the number of samples processed so far.\n",
    "\n",
    "print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "Finally, this line prints the loss and the progress (number of samples processed out of the total).\n",
    "\n",
    "In summary, this function iteratively updates the model's parameters to minimize the loss function, thereby training the model on the dataset. The training process involves making predictions, calculating the loss, performing backpropagation to compute gradients, and then updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model’s performance against the test dataset to ensure it is learning.\n",
    "\n",
    "This code snippet defines a function for evaluating the performance of your neural network on a test dataset. Testing on a separate dataset that wasn't used during training is crucial for assessing how well your model generalizes to new, unseen data. \n",
    "\n",
    "step by step:\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "\n",
    "This line defines the test function with three parameters:\n",
    "\n",
    "dataloader: This provides batches of test data.\n",
    "model: The neural network model that you're evaluating.\n",
    "loss_fn: The loss function used to evaluate the model's performance.\n",
    "size = len(dataloader.dataset)\n",
    "\n",
    "This calculates the total number of samples in the test dataset.\n",
    "\n",
    "num_batches = len(dataloader)\n",
    "\n",
    "This calculates the total number of batches in the dataloader.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "Sets the model to evaluation mode. This is important for certain types of layers (like dropout layers or batch normalization layers) that behave differently during training and testing.\n",
    "\n",
    "test_loss, correct = 0, 0\n",
    "\n",
    "Initializes variables to track the total test loss and the number of correct predictions.\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "This context manager tells PyTorch that gradient computation is not needed in this block. This is because during testing, we don't need to update the weights of the model, and thus, don't need the gradients.\n",
    "\n",
    "for X, y in dataloader:\n",
    "\n",
    "Iterates over the test dataloader. In each iteration, it provides a batch of data (X) and the corresponding labels (y).\n",
    "\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "Moves the data and labels to the specified device.\n",
    "\n",
    "pred = model(X)\n",
    "\n",
    "The model makes predictions based on the batch of data X.\n",
    "\n",
    "test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "Adds up the loss for each batch. .item() converts the loss tensor to a Python number.\n",
    "\n",
    "correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "Counts the number of correct predictions. pred.argmax(1) gets the index of the highest value in each prediction (which represents the predicted class). This is compared to the actual labels y. The sum of correct predictions is accumulated.\n",
    "\n",
    "test_loss /= num_batches\n",
    "\n",
    "Calculates the average test loss by dividing the total loss by the number of batches.\n",
    "\n",
    "correct /= size\n",
    "\n",
    "Calculates the accuracy by dividing the number of correct predictions by the total number of samples.\n",
    "\n",
    "print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "Prints out the test accuracy and the average test loss.\n",
    "\n",
    "This function is a standard way to evaluate the performance of a neural network. It provides a good indication of how well the model is likely to perform on unseen data, which is a critical aspect of building reliable machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch.\n",
    "\n",
    "step-by-step:\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "This sets the number of epochs to 5. You will train and test your model five times over the entire dataset.\n",
    "\n",
    "for t in range(epochs):\n",
    "\n",
    "This is a loop that will iterate five times (once for each epoch).\n",
    "\n",
    "print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "This prints the current epoch number. t+1 is used because t starts from 0, so t+1 gives you the human-readable epoch number (starting from 1).\n",
    "\n",
    "train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "This calls the train function defined earlier. It will train the model using the training dataset (train_dataloader) and the specified loss function and optimizer. The training process involves passing the training data through the model, calculating the loss, performing backpropagation to update the model's weights based on this loss, and iterating over the entire training dataset.\n",
    "\n",
    "test(test_dataloader, model, loss_fn)\n",
    "\n",
    "After each training epoch, this line calls the test function. The test function evaluates the model's performance on the test dataset (test_dataloader). This is critical for understanding how well your model is learning and generalizing to new data.\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "This prints \"Done!\" after all epochs are completed, indicating the end of the training and testing process.\n",
    "\n",
    "The purpose of iterating over multiple epochs is to allow the model to learn from the dataset effectively. In each epoch, the model has an opportunity to adjust its weights and biases to improve its predictions. The test after each epoch gives you insights into whether the model is improving, overfitting (performing well on training data but poorly on test data), or underfitting (performing poorly on both).\n",
    "\n",
    "The number of epochs, 5 in this case, is a hyperparameter that you can adjust. The optimal number of epochs varies depending on the specific dataset and model architecture. Too few epochs might underfit the model, while too many might lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303594  [   64/60000]\n",
      "loss: 2.292017  [ 6464/60000]\n",
      "loss: 2.270508  [12864/60000]\n",
      "loss: 2.264066  [19264/60000]\n",
      "loss: 2.245029  [25664/60000]\n",
      "loss: 2.222055  [32064/60000]\n",
      "loss: 2.232006  [38464/60000]\n",
      "loss: 2.197491  [44864/60000]\n",
      "loss: 2.194942  [51264/60000]\n",
      "loss: 2.181595  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.160689 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.169685  [   64/60000]\n",
      "loss: 2.164948  [ 6464/60000]\n",
      "loss: 2.098836  [12864/60000]\n",
      "loss: 2.116326  [19264/60000]\n",
      "loss: 2.070799  [25664/60000]\n",
      "loss: 2.015916  [32064/60000]\n",
      "loss: 2.047490  [38464/60000]\n",
      "loss: 1.967980  [44864/60000]\n",
      "loss: 1.968687  [51264/60000]\n",
      "loss: 1.924091  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.898909 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.928288  [   64/60000]\n",
      "loss: 1.908277  [ 6464/60000]\n",
      "loss: 1.775222  [12864/60000]\n",
      "loss: 1.819747  [19264/60000]\n",
      "loss: 1.723400  [25664/60000]\n",
      "loss: 1.674419  [32064/60000]\n",
      "loss: 1.700653  [38464/60000]\n",
      "loss: 1.602318  [44864/60000]\n",
      "loss: 1.619880  [51264/60000]\n",
      "loss: 1.535531  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.536175 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.595168  [   64/60000]\n",
      "loss: 1.574242  [ 6464/60000]\n",
      "loss: 1.411343  [12864/60000]\n",
      "loss: 1.486351  [19264/60000]\n",
      "loss: 1.373798  [25664/60000]\n",
      "loss: 1.367595  [32064/60000]\n",
      "loss: 1.382450  [38464/60000]\n",
      "loss: 1.313018  [44864/60000]\n",
      "loss: 1.339881  [51264/60000]\n",
      "loss: 1.249973  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.269687 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.339761  [   64/60000]\n",
      "loss: 1.332863  [ 6464/60000]\n",
      "loss: 1.160226  [12864/60000]\n",
      "loss: 1.263682  [19264/60000]\n",
      "loss: 1.139717  [25664/60000]\n",
      "loss: 1.164908  [32064/60000]\n",
      "loss: 1.183789  [38464/60000]\n",
      "loss: 1.130855  [44864/60000]\n",
      "loss: 1.160072  [51264/60000]\n",
      "loss: 1.079891  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.099608 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is about saving the trained state of your PyTorch model. Saving a model in PyTorch is a crucial step as it allows you to load the model later for further training, evaluation, or to make predictions on new data. Let's break it down:\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "torch.save: This is a PyTorch function used for serializing Python objects. In the context of PyTorch models, it's typically used to save the model's parameters.\n",
    "model.state_dict(): This is a PyTorch function that returns a Python dictionary containing all the weights and biases of the model. The state dict is a snapshot of the model's parameters at a particular point in time.\n",
    "\"model.pth\": This is the filename for the saved model. The .pth extension is a convention used for PyTorch model files, but you could use any other filename or extension if you prefer.\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "This line simply prints a confirmation message indicating that the model has been saved successfully.\n",
    "\n",
    "When you save a model using state_dict, you're only saving the parameters of the model, not the entire model or its architecture. This is efficient because it reduces the size of the saved model file. However, when you want to load the model for further use, you'll need to recreate the model architecture in code and load the saved parameters into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final code snippet demonstrates how to use your trained PyTorch model to make a prediction on a single data sample from the test dataset. It also includes a nice way of presenting the model's prediction and the actual label for human readability. Let's go through it:\n",
    "\n",
    "classes = [...]\n",
    "\n",
    "This list defines the classes for your classification problem. Each class corresponds to a label that the model can predict. These labels represent different types of clothing items.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "Sets the model to evaluation mode. This is important as certain layers in your model might behave differently during training (like dropout layers), and you want to evaluate the model in its testing configuration.\n",
    "\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "This line retrieves the first sample ([0]) from your test dataset. x is the image data, and y is the corresponding label.\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "Temporarily sets all the gradients to zero. This is important because during inference (making predictions), you typically don't want to perform backpropagation, and hence you don't need gradients.\n",
    "\n",
    "x = x.to(device)\n",
    "\n",
    "Moves the input data x to the specified device (CPU or GPU), making it ready for the model to process.\n",
    "\n",
    "pred = model(x)\n",
    "\n",
    "Feeds the input data x through the model to get a prediction. The output pred is a tensor of class probabilities.\n",
    "\n",
    "predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "\n",
    "pred[0].argmax(0): This finds the index of the highest value in the predictions tensor, which corresponds to the most likely class predicted by the model.\n",
    "classes[...]: It then uses this index to find the corresponding class label in the classes list.\n",
    "classes[y]: Retrieves the actual label for the data sample.\n",
    "print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "\n",
    "Finally, it prints out the model's predicted class and the actual class for the data sample. This is useful for visually inspecting how well your model is performing on individual examples.\n",
    "\n",
    "This snippet is often used to quickly test a model's prediction on a specific sample or to showcase the model's capabilities. It provides a clear and concise way to compare the model's prediction against the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
